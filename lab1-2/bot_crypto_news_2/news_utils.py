#!/usr/bin/env python3
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
–ú–æ–¥—É–ª—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
"""

import requests
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import json
import os
from urllib.parse import urljoin, urlparse
import xml.etree.ElementTree as ET

logger = logging.getLogger(__name__)

class NewsFetcher:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        self.news_api_key = os.getenv('NEWSAPI_KEY')
        self.mediastack_api_key = os.getenv('MEDIASTACK_API_KEY')
        self.http_timeout = int(os.getenv('HTTP_TIMEOUT', 30))
        
        # RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        self.rss_feeds = {
            'ru': [
                'https://lenta.ru/rss',
                'https://ria.ru/export/rss2/archive/index.xml',
                'https://www.interfax.ru/rss.asp',
                'https://www.vedomosti.ru/rss/news',
                'https://www.gazeta.ru/export/rss/lenta.xml'
            ],
            'en': [
                'https://feeds.bbci.co.uk/news/rss.xml',
                'https://rss.cnn.com/rss/edition.rss',
                'https://feeds.reuters.com/reuters/topNews',
                'https://feeds.npr.org/1001/rss.xml'
            ]
        }
    
    def fetch_rss_news(self, language: str = 'ru', limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS –ª–µ–Ω—Ç"""
        news_list = []
        
        try:
            feeds = self.rss_feeds.get(language, self.rss_feeds['ru'])
            
            for feed_url in feeds[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                try:
                    # –ü–æ–ª—É—á–∞–µ–º RSS –ª–µ–Ω—Ç—É
                    response = requests.get(feed_url, timeout=self.http_timeout)
                    response.raise_for_status()
                    
                    # –ü–∞—Ä—Å–∏–º XML
                    root = ET.fromstring(response.content)
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º namespace –¥–ª—è RSS
                    namespaces = {
                        'rss': 'http://purl.org/rss/1.0/',
                        'atom': 'http://www.w3.org/2005/Atom',
                        'dc': 'http://purl.org/dc/elements/1.1/',
                        'content': 'http://purl.org/rss/1.0/modules/content/'
                    }
                    
                    # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π
                    items = []
                    
                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã RSS
                    if root.tag == 'rss':
                        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π RSS 2.0
                        channel = root.find('channel')
                        if channel is not None:
                            items = channel.findall('item')
                    elif root.tag.endswith('feed'):
                        # Atom feed
                        items = root.findall('atom:entry', namespaces)
                    else:
                        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞–ø—Ä—è–º—É—é
                        items = root.findall('.//item') or root.findall('.//entry')
                    
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ channel, –ø—Ä–æ–±—É–µ–º –Ω–∞–ø—Ä—è–º—É—é
                    if not items:
                        items = root.findall('.//item') or root.findall('.//entry')
                    
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
                    items_to_process = min(len(items), max(1, limit // len(feeds)))
                    
                    for item in items[:items_to_process]:
                        try:
                            news_item = self._parse_rss_item(item, namespaces, feed_url, language)
                            if news_item:
                                news_list.append(news_item)
                        except Exception as e:
                            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ RSS: {e}")
                            continue
                            
                except ET.ParseError as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML –¥–ª—è {feed_url}: {e}")
                    continue
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RSS {feed_url}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è RSS –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        
        return news_list[:limit]
    
    def _parse_rss_item(self, item: ET.Element, namespaces: Dict[str, str], feed_url: str, language: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ RSS"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title = None
            title_elem = item.find('title')
            if title_elem is not None:
                title = title_elem.text.strip() if title_elem.text else None
            
            logger.debug(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = None
            desc_elem = item.find('description') or item.find('summary')
            if desc_elem is not None:
                description = desc_elem.text.strip() if desc_elem.text else None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É
            url = None
            link_elem = item.find('link')
            if link_elem is not None:
                url = link_elem.text.strip() if link_elem.text else None
                # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–∞—è, –¥–µ–ª–∞–µ–º –µ—ë –∞–±—Å–æ–ª—é—Ç–Ω–æ–π
                if url and not url.startswith('http'):
                    url = urljoin(feed_url, url)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
            date_str = None
            date_elem = item.find('pubDate') or item.find('published') or item.find('dc:date', namespaces)
            if date_elem is not None:
                date_str = date_elem.text.strip() if date_elem.text else None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if not title:
                return None
            
            result = {
                'title': title,
                'description': description or '',
                'url': url or '',
                'date': self._parse_date(date_str),
                'source': self._extract_domain(feed_url),
                'language': language
            }
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS —ç–ª–µ–º–µ–Ω—Ç–∞: {e}")
            return None
    
    def fetch_api_news(self, language: str = 'ru', country: str = 'ru', limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ News API"""
        news_list = []
        
        if not self.news_api_key:
            logger.warning("News API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return news_list
        
        try:
            url = 'https://newsapi.org/v2/top-headlines'
            params = {
                'apiKey': self.news_api_key,
                'country': country,
                'language': language,
                'pageSize': limit
            }
            
            response = requests.get(url, params=params, timeout=self.http_timeout)
            response.raise_for_status()
            
            data = response.json()
            
            for article in data.get('articles', []):
                news_item = {
                    'title': article.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'),
                    'description': article.get('description', ''),
                    'url': article.get('url', ''),
                    'date': self._parse_date(article.get('publishedAt', '')),
                    'source': article.get('source', {}).get('name', 'Unknown'),
                    'language': language
                }
                news_list.append(news_item)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ News API: {e}")
        
        return news_list
    
    def fetch_mediastack_news(self, language: str = 'ru', countries: str = 'ru', limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Mediastack API"""
        news_list = []
        
        if not self.mediastack_api_key:
            logger.warning("Mediastack API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return news_list
        
        try:
            url = 'http://api.mediastack.com/v1/news'
            params = {
                'access_key': self.mediastack_api_key,
                'languages': language,
                'countries': countries,
                'limit': limit
            }
            
            response = requests.get(url, params=params, timeout=self.http_timeout)
            response.raise_for_status()
            
            data = response.json()
            
            for article in data.get('data', []):
                news_item = {
                    'title': article.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'),
                    'description': article.get('description', ''),
                    'url': article.get('url', ''),
                    'date': self._parse_date(article.get('published_at', '')),
                    'source': article.get('source', 'Unknown'),
                    'language': language
                }
                news_list.append(news_item)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Mediastack: {e}")
        
        return news_list
    
    def get_news_by_topics(self, topics: List[str], language: str = 'ru', limit: int = 10) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ç–µ–º–∞–º"""
        all_news = []
        
        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        all_news.extend(self.fetch_rss_news(language, limit))
        all_news.extend(self.fetch_api_news(language, 'ru', limit))
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ç–µ–º–∞–º
        filtered_news = []
        for news in all_news:
            title_lower = news['title'].lower()
            description_lower = news.get('description', '').lower()
            
            for topic in topics:
                topic_lower = topic.lower()
                if (topic_lower in title_lower or 
                    topic_lower in description_lower):
                    filtered_news.append(news)
                    break
        
        return filtered_news[:limit]
    
    def _parse_date(self, date_str: str) -> str:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        try:
            if not date_str:
                return datetime.now().strftime('%d.%m.%Y %H:%M')
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
            formats = [
                '%Y-%m-%dT%H:%M:%SZ',
                '%Y-%m-%dT%H:%M:%S.%fZ',
                '%a, %d %b %Y %H:%M:%S %Z',
                '%Y-%m-%d %H:%M:%S'
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str, fmt)
                    return dt.strftime('%d.%m.%Y %H:%M')
                except ValueError:
                    continue
            
            return datetime.now().strftime('%d.%m.%Y %H:%M')
            
        except Exception:
            return datetime.now().strftime('%d.%m.%Y %H:%M')
    
    def _extract_domain(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–º–µ–Ω–∞ –∏–∑ URL"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except Exception:
            return 'Unknown'


class NewsFilter:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏–ª—å—Ç—Ä–∞"""
        self.keywords_blacklist = [
            '—Ä–µ–∫–ª–∞–º–∞', 'advertisement', 'sponsored', '–ø—Ä–æ–º–æ',
            '–∞–∫—Ü–∏—è', '—Å–∫–∏–¥–∫–∞', 'sale', 'discount'
        ]
    
    def filter_news(self, news_list: List[Dict[str, Any]], 
                   keywords: List[str] = None,
                   exclude_keywords: List[str] = None) -> List[Dict[str, Any]]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        filtered_news = []
        
        for news in news_list:
            title_lower = news['title'].lower()
            description_lower = news.get('description', '').lower()
            text = f"{title_lower} {description_lower}"
            
            # –ò—Å–∫–ª—é—á–∞–µ–º —Ä–µ–∫–ª–∞–º—É
            if any(keyword in text for keyword in self.keywords_blacklist):
                continue
            
            # –ò—Å–∫–ª—é—á–∞–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            if exclude_keywords:
                if any(keyword.lower() in text for keyword in exclude_keywords):
                    continue
            
            # –í–∫–ª—é—á–∞–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            if keywords:
                if any(keyword.lower() in text for keyword in keywords):
                    filtered_news.append(news)
            else:
                filtered_news.append(news)
        
        return filtered_news
    
    def remove_duplicates(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            title = news['title'].lower().strip()
            if title not in seen_titles:
                seen_titles.add(title)
                unique_news.append(news)
        
        return unique_news


class NewsFormatter:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    @staticmethod
    def format_news_list(news_list: List[Dict[str, Any]], 
                        max_items: int = 10) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏"""
        if not news_list:
            return "üì∞ –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."
        
        message = f"üì∞ –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news_list)}\n\n"
        
        for i, news in enumerate(news_list[:max_items], 1):
            message += f"{i}. {news['title']}\n"
            
            if news.get('description'):
                description = news['description'][:100]
                if len(news['description']) > 100:
                    description += "..."
                message += f"   üìù {description}\n"
            
            message += f"   üìÖ {news['date']}\n"
            message += f"   üì° {news.get('source', 'Unknown')}\n"
            
            if news.get('url'):
                message += f"   üîó {news['url']}\n"
            
            message += "\n"
        
        return message
    
    @staticmethod
    def format_digest(news_list: List[Dict[str, Any]], 
                     date: str = None) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–π–¥–∂–µ—Å—Ç–∞"""
        if not date:
            date = datetime.now().strftime('%d.%m.%Y')
        
        message = f"üìß –í–∞—à –¥–∞–π–¥–∂–µ—Å—Ç –∑–∞ {date}:\n\n"
        
        if not news_list:
            message += "üì∞ –ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
            return message
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        sources = {}
        for news in news_list:
            source = news.get('source', 'Unknown')
            if source not in sources:
                sources[source] = []
            sources[source].append(news)
        
        for source, source_news in sources.items():
            message += f"üì° {source}:\n"
            for news in source_news[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                message += f"‚Ä¢ {news['title']}\n"
            message += "\n"
        
        message += "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /top –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –ø–æ–ª–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π."
        
        return message
